ratio.real <-
samples <-
ggplot() +
geom_density(aes(samples)) +
geom_vline(aes(xintercept=ratio.real), color="red")
# Librerias
# Leer información
data_notas <- read.csv("notas.csv")
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu,sigma){
.... # Funcion de likelihood
}
# Valores de la grilla
grid_mu <- ...
poke.df <- read.csv("Pokemon.csv")
modelo <- alist(
Sp..Atk ~ dnorm(mu, sigma),
mu <- alpha + beta * Attack,
alpha ~ dnorm(100, 20),   # Prior para alpha
beta ~ dnorm(0, 10),      # Prior para beta
sigma ~ dexp(1)           # Prior para sigma
)
fit <- map(
alist(
Sp..Atk ~ dnorm(mu, sigma),
mu <- alpha + beta * Attack,
alpha ~ dnorm(100, 20),
beta ~ dnorm(0, 10),
sigma ~ dexp(1)
),
data = poke.df
)
precis(fit)
plot_data <- data.frame(Attack = seq(min(poke.df$Attack), max(poke.df$Attack), length.out = 100))
plot_data$mu <- coef(fit)["alpha"] + coef(fit)["beta"] * plot_data$Attack
ggplot() +
geom_point(aes(x=poke.df$Attack, y=poke.df$Sp..Atk), color="red") +
geom_line(aes(x=, y=), color="orange") +
geom_ribbon(aes(x=, y=, ymin=, ymax=), alpha=0.3, fill="orange") +
geom_ribbon(aes(x=, y=, ymin=, ymax=), alpha=0.1, fill="orange")
plot_data <- data.frame(Attack = seq(min(poke.df$Attack), max(poke.df$Attack), length.out = 100))
plot_data$mu <- coef(fit)["alpha"] + coef(fit)["beta"] * plot_data$Attack
ggplot() +
geom_point(aes(x = poke.df$Attack, y = poke.df$Sp..Atk), color = "red") +
geom_line(aes(x = plot_data$Attack, y = plot_data$mu), color = "orange") +
geom_ribbon(aes(x = plot_data$Attack, ymin = plot_data$mu - 1.96 * coef(fit)["sigma"],
ymax = plot_data$mu + 1.96 * coef(fit)["sigma"]), alpha = 0.3, fill = "orange")
plot_data <- data.frame(Attack = seq(min(poke.df$Attack), max(poke.df$Attack), length.out = 100))
plot_data$mu <- coef(fit)["alpha"] + coef(fit)["beta"] * plot_data$Attack
ggplot() +
geom_point(aes(x=poke.df$Attack, y=poke.df$Sp..Atk), color="red") +
geom_line(aes(x=, y=), color="orange") +
geom_ribbon(aes(x=, y=, ymin=, ymax=), alpha=0.3, fill="orange") +
geom_ribbon(aes(x=, y=, ymin=, ymax=), alpha=0.1, fill="orange")
poke.df <- read.csv("Pokemon.csv")
head(poke.df)
plot_data <- data.frame(Attack = seq(min(poke.df$Attack), max(poke.df$Attack), length.out = 100))
plot_data$mu <- coef(fit)["alpha"] + coef(fit)["beta"] * plot_data$Attack
ggplot() +
geom_point(aes(x = poke.df$Attack, y = poke.df$Sp..Atk), color = "red") +
geom_line(aes(x = attack_seq, y = mu_pred), color = "orange") +
geom_ribbon(aes(x = attack_seq, ymin = mu_pred - 1.96 * coef(fit)["sigma"],
ymax = mu_pred + 1.96 * coef(fit)["sigma"]), alpha = 0.3, fill = "orange") +
theme_minimal() +
labs(x = "Attack", y = "Sp. Atk", title = "Regresión Lineal Bayesiana con Intervalos de Credibilidad")
plot_data <- data.frame(Attack = seq(min(poke.df$Attack), max(poke.df$Attack), length.out = 100))
plot_data$mu <- coef(fit)["alpha"] + coef(fit)["beta"] * plot_data$Attack
ggplot() +
geom_point(aes(x = poke.df$Attack, y = poke.df$Sp..Atk), color = "red") +
geom_line(aes(x = plot_data$Attack, y = plot_data$mu), color = "orange") +
geom_ribbon(aes(x = plot_data$Attack, ymin = plot_data$mu - 1.96 * coef(fit)["sigma"],
ymax = plot_data$mu + 1.96 * coef(fit)["sigma"]), alpha = 0.3, fill = "orange")
modelo <- alist(
Sp..Atk ~ dnorm(mu, sigma),
mu <- alpha + beta * Attack,
alpha ~ dnorm(100, 20),   # Prior para alpha
beta ~ dnorm(0, 10),      # Prior para beta
sigma ~ dexp(1)           # Prior para sigma
)
fit <- map(
alist(
Sp..Atk ~ dnorm(mu, sigma),
mu <- alpha + beta * Attack,
alpha ~ dnorm(100, 20),
beta ~ dnorm(0, 10),
sigma ~ dexp(1)
),
data = poke.df
)
precis(fit)
fit <- map(
alist(
Sp..Atk ~ dnorm(mu, sigma),
mu <- alpha + beta * Attack,
alpha ~ dnorm(100, 20),
beta ~ dnorm(0, 10),
sigma ~ dexp(1)
),
data = poke.df
)
precis(fit)
plot_data <- data.frame(Attack = seq(min(poke.df$Attack), max(poke.df$Attack), length.out = 100))
plot_data$mu <- coef(fit)["alpha"] + coef(fit)["beta"] * plot_data$Attack
ggplot() +
geom_point(aes(x = poke.df$Attack, y = poke.df$Sp..Atk), color = "red") +
geom_line(aes(x = plot_data$Attack, y = plot_data$mu), color = "orange") +
geom_ribbon(aes(x = plot_data$Attack, ymin = plot_data$mu - 1.96 * coef(fit)["sigma"],
ymax = plot_data$mu + 1.96 * coef(fit)["sigma"]), alpha = 0.3, fill = "orange")
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$nota, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$nota) - 3, mean(data_notas$nota) + 3, length.out = 100)
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas).co
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas).col
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas).col()
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas).col(1)
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas).col
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$nota, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$nota) - 3, mean(data_notas$nota) + 3, length.out = 100)
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$Notas, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$Notas) - 3, mean(data_notas$Notas) + 3, length.out = 100)
grid_sigma <- seq(0.5, 1.5, length.out = 100)
# Se crea la grilla 2D
data_grid <- expand_grid(grid_mu, grid_sigma)
# Se guarda la likelihood
data_grid <- data_grid %>%
mutate(likelihood = map2_dbl(grid_mu, grid_sigma, grid_function))
# Priors
prior_mu <- rep(1, length(grid_mu)) # Uniforme para mu
prior_sigma <- ifelse(grid_sigma <= 1, 0.75, 0.25) # Ponderación más alta para [0.5, 1]
# Normalización de priors
prior_mu <- prior_mu / sum(prior_mu)
prior_sigma <- prior_sigma / sum(prior_sigma)
# Asignar priors al grid
data_grid <- data_grid %>%
mutate(prior_mu = rep(prior_mu, each = length(grid_sigma)),
prior_sigma = rep(prior_sigma, length(grid_mu)),
prior = prior_mu * prior_sigma)
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$Notas, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$Notas) - 3, mean(data_notas$Notas) + 3, length.out = 100)
grid_sigma <- seq(0.5, 1.5, length.out = 100)
# Se crea la grilla 2D
data_grid <- expand_grid(grid_mu, grid_sigma)
# Se guarda la likelihood
data_grid <- data_grid %>%
mutate(likelihood = map2_dbl(grid_mu, grid_sigma, grid_function))
# Priors
prior_mu <- rep(1, length(grid_mu)) # Uniforme para mu
prior_sigma <- ifelse(grid_sigma <= 1, 0.75, 0.25) # Ponderación más alta para [0.5, 1]
# Normalización de priors
prior_mu <- prior_mu / sum(prior_mu)
prior_sigma <- prior_sigma / sum(prior_sigma)
# Asignar priors al grid
data_grid <- data_grid %>%
mutate(prior_mu = rep(prior_mu, each = length(grid_sigma)),
prior_sigma = rep(prior_sigma, length(grid_mu)),
prior = prior_mu * prior_sigma)
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$Notas, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$Notas) - 3, mean(data_notas$Notas) + 3, length.out = 100)
grid_sigma <- seq(0.5, 1.5, length.out = 100)
# Verificar el cálculo de la media
print(mu_mean)
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$Notas, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$Notas) - 3, mean(data_notas$Notas) + 3, length.out = 100)
grid_sigma <- seq(0.5, 1.5, length.out = 100)
# Se crea la grilla 2D
data_grid <- expand_grid(grid_mu, grid_sigma)
# Se guarda la likelihood
data_grid <- data_grid %>%
mutate(likelihood = map2_dbl(grid_mu, grid_sigma, grid_function))
# Priors
prior_mu <- rep(1, length(grid_mu)) # Uniforme para mu
prior_sigma <- ifelse(grid_sigma <= 1, 0.75, 0.25) # Ponderación más alta para [0.5, 1]
# Normalización de priors
prior_mu <- prior_mu / sum(prior_mu)
prior_sigma <- prior_sigma / sum(prior_sigma)
# Asignar priors al grid
data_grid <- data_grid %>%
mutate(prior_mu = rep(prior_mu, each = length(grid_sigma)),
prior_sigma = rep(prior_sigma, length(grid_mu)),
prior = prior_mu * prior_sigma)
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$Notas, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$Notas, na.rm = TRUE) - 3,
mean(data_notas$Notas, na.rm = TRUE) + 3, length.out = 100)
grid_sigma <- seq(0.5, 1.5, length.out = 100)
# Crear grilla 2D
data_grid <- expand_grid(grid_mu, grid_sigma)
# Calcular likelihood para cada par (mu, sigma)
data_grid <- data_grid %>%
mutate(likelihood = map2_dbl(grid_mu, grid_sigma, grid_function))
# Priors
prior_mu <- rep(1 / length(grid_mu), length(grid_mu)) # Uniforme para mu
prior_sigma <- ifelse(grid_sigma <= 1, 0.75, 0.25)    # Ponderación más alta para [0.5, 1]
prior_sigma <- prior_sigma / sum(prior_sigma)         # Normalizar prior_sigma
# Combinar priors en la grilla
data_grid <- data_grid %>%
mutate(prior = rep(prior_mu, times = length(grid_sigma)) *
rep(prior_sigma, each = length(grid_mu)))
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$Notas, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$Notas, na.rm = TRUE) - 3,
mean(data_notas$Notas, na.rm = TRUE) + 3, length.out = 100)
grid_sigma <- seq(0.5, 1.5, length.out = 100)
# Crear grilla 2D
data_grid <- expand_grid(grid_mu, grid_sigma)
# Calcular likelihood para cada par (mu, sigma)
data_grid <- data_grid %>%
mutate(likelihood = map2_dbl(grid_mu, grid_sigma, grid_function))
# Priors
prior_mu <- rep(1 / length(grid_mu), length(grid_mu)) # Uniforme para mu
prior_sigma <- ifelse(grid_sigma <= 1, 0.75, 0.25)    # Ponderación más alta para [0.5, 1]
prior_sigma <- prior_sigma / sum(prior_sigma)         # Normalizar prior_sigma
# Crear columnas separadas para priors en data_grid
data_grid <- data_grid %>%
mutate(prior_mu = prior_mu[match(grid_mu, unique(grid_mu))],
prior_sigma = prior_sigma[match(grid_sigma, unique(grid_sigma))],
prior = prior_mu * prior_sigma)
# Calcular posterior no estandarizado
data_grid <- data_grid %>%
mutate(unstd_posterior = likelihood + log(prior), # Log-transformación para estabilidad numérica
posterior = exp(unstd_posterior - max(unstd_posterior))) %>% # Ajuste para evitar underflow
mutate(posterior = posterior / sum(posterior)) # Normalización del posterior
# Mostrar las primeras filas del dataframe resultante
head(data_grid)
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$Notas, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$Notas, na.rm = TRUE) - 3,
mean(data_notas$Notas, na.rm = TRUE) + 3, length.out = 100)
grid_sigma <- seq(0.5, 1.5, length.out = 100)
# Crear grilla 2D
data_grid <- expand_grid(grid_mu, grid_sigma)
# Calcular likelihood para cada par (mu, sigma)
data_grid <- data_grid %>%
mutate(likelihood = map2_dbl(grid_mu, grid_sigma, grid_function))
# Priors
prior_mu <- rep(1 / length(grid_mu), length(grid_mu)) # Uniforme para mu
prior_sigma <- ifelse(grid_sigma <= 1, 0.75, 0.25)    # Ponderación más alta para [0.5, 1]
prior_sigma <- prior_sigma / sum(prior_sigma)         # Normalizar prior_sigma
# Crear columnas separadas para priors en data_grid
data_grid <- data_grid %>%
mutate(prior_mu = prior_mu[match(grid_mu, unique(grid_mu))],
prior_sigma = prior_sigma[match(grid_sigma, unique(grid_sigma))],
prior = prior_mu * prior_sigma)
# Calcular posterior no estandarizado
data_grid <- data_grid %>%
mutate(unstd_posterior = likelihood + log(prior), # Log-transformación para estabilidad numérica
posterior = exp(unstd_posterior - max(unstd_posterior))) %>% # Ajuste para evitar underflow
mutate(posterior = posterior / sum(posterior)) # Normalización del posterior
# Mostrar las primeras filas del dataframe resultante
head(data_grid)
# Punto de referencia
# Se recomienda cambiar estos valores por unos adecuados que le permitan estudiar
# Los valores de la distribución de mejor manera
valor_x <- 1
valor_y <- 1
# Grafico
punto_comparacion <- tibble(x = valor_x, y = valor_y)
plt <- data_grid %>%
ggplot(aes(x = grid_mu, y = grid_sigma)) +
geom_raster(aes(fill = posterior),
interpolate = T
)+
geom_point(x = valor_x, y = valor_y, size = 1.3,color="white")+
geom_label(
data = punto_comparacion, aes(x, y),
label = "Punto Comparación",
fill = "green",
color = "black",
nudge_y = 0, # Este parametro desplaza la caja por el eje y
nudge_x = 1 # Este parametro desplaza la caja por el eje x
)+
scale_fill_viridis_c() +
labs(
title = "Posterior para Mean y Standard Deviation",
x = expression(mu ["Mean"]),
y = expression(sigma ["Standar Deviation"])
) +
theme(panel.grid = element_blank())
plt
# Codificamos los datos
x <- 1:length(data_grid$posterior)
# Sampleamos los indices
posterior_samples_aux <- sample(x,size = 1e4, replace = T, prob = data_grid$posterior)
# Obtenemos los verdaderos valores de la sampling distribution
posterior_samples <- data_grid[posterior_samples_aux,]
# Obtenemos solos los valores relevantes para la densidad
df <- data.frame(posterior_samples$grid_mu,posterior_samples$grid_sigma)
# Realizamos las densidades
dens(df)
# Manipulación de estructuras
library(tidyverse)
library(dplyr)
library(tidyr)
library(purrr)
# Para realizar plots
library(scatterplot3d)
library(ggplot2)
library(plotly)
# Manipulación de varios plots en una imagen.
library(gridExtra)
# Análisis bayesiano
library("rethinking")
install.packages("rethinking")
install.packages(c("mvtnorm","loo","coda"), repos="https://cloud.r-project.org/",dependencies=TRUE)
options(repos=c(getOption('repos'), rethinking='http://xcelab.net/R'))
install.packages('rethinking',type='source')
dataMordidas <- read.csv("no_mordidas.csv", header = TRUE)
cols <- c("bites_month_1", "bites_month_2", "bites_month_3",
"bites_month_4", "bites_month_5", "bites_month_6",
"bites_month_7", "bites_month_8")
grid_len <- 100
p_grid <-
prior <-
posteriors <- c()
counted.months <- c()
p_grids <- rep(p_grid, 4)
for (n in c(1, 2, 4, 8)) {
x <- # total de mordidas
likelihood <-
unstd.posterior <-
posterior <-
posteriors <- c(posteriors, posterior)
counted.months <- c(counted.months, rep(n, grid_len))
}
# Manipulación de estructuras
library(tidyverse)
library(dplyr)
library(tidyr)
library(purrr)
# Para realizar plots
library(scatterplot3d)
library(ggplot2)
library(plotly)
# Manipulación de varios plots en una imagen.
library(gridExtra)
# Análisis bayesiano
library("rethinking")
install.packages("rethinking")
install.packages(c("mvtnorm","loo","coda"), repos="https://cloud.r-project.org/",dependencies=TRUE)
options(repos=c(getOption('repos'), rethinking='http://xcelab.net/R'))
install.packages('rethinking',type='source')
dataMordidas <- read.csv("no_mordidas.csv", header = TRUE)
cols <- c("bites_month_1", "bites_month_2", "bites_month_3",
"bites_month_4", "bites_month_5", "bites_month_6",
"bites_month_7", "bites_month_8")
grid_len <- 100
p_grid <-
prior <-
posteriors <- c()
counted.months <- c()
p_grids <- rep(p_grid, 4)
for (n in c(1, 2, 4, 8)) {
x <- # total de mordidas
likelihood <-
unstd.posterior <-
posterior <-
posteriors <- c(posteriors, posterior)
counted.months <- c(counted.months, rep(n, grid_len))
}
install.packages(c("mvtnorm", "loo", "coda"), repos = "https://cloud.r-project.org/", dependencies = TRUE)
install.packages("rethinking")
install.packages("rethinking", type = "source")
# Leer información
data_notas <- read.csv("notas.csv")
head(data_notas)
# Función para crear likelihood dado mu y sigma
grid_function <- function(mu, sigma) {
# Log-likelihood basado en distribución normal
sum(dnorm(data_notas$Notas, mean = mu, sd = sigma, log = TRUE))
}
# Valores de la grilla
grid_mu <- seq(mean(data_notas$Notas, na.rm = TRUE) - 3,
mean(data_notas$Notas, na.rm = TRUE) + 3, length.out = 100)
grid_sigma <- seq(0.5, 1.5, length.out = 100)
# Crear grilla 2D
data_grid <- expand_grid(grid_mu, grid_sigma)
# Calcular likelihood para cada par (mu, sigma)
data_grid <- data_grid %>%
mutate(likelihood = map2_dbl(grid_mu, grid_sigma, grid_function))
# Priors
prior_mu <- rep(1 / length(grid_mu), length(grid_mu)) # Uniforme para mu
prior_sigma <- ifelse(grid_sigma <= 1, 0.75, 0.25)    # Ponderación más alta para [0.5, 1]
prior_sigma <- prior_sigma / sum(prior_sigma)         # Normalizar prior_sigma
# Crear columnas separadas para priors en data_grid
data_grid <- data_grid %>%
mutate(prior_mu = prior_mu[match(grid_mu, unique(grid_mu))],
prior_sigma = prior_sigma[match(grid_sigma, unique(grid_sigma))],
prior = prior_mu * prior_sigma)
# Calcular posterior no estandarizado
data_grid <- data_grid %>%
mutate(unstd_posterior = likelihood + log(prior), # Log-transformación para estabilidad numérica
posterior = exp(unstd_posterior - max(unstd_posterior))) %>% # Ajuste para evitar underflow
mutate(posterior = posterior / sum(posterior)) # Normalización del posterior
# Mostrar las primeras filas del dataframe resultante
head(data_grid)
# Punto de referencia
# Se recomienda cambiar estos valores por unos adecuados que le permitan estudiar
# Los valores de la distribución de mejor manera
valor_x <- 1
valor_y <- 1
# Grafico
punto_comparacion <- tibble(x = valor_x, y = valor_y)
plt <- data_grid %>%
ggplot(aes(x = grid_mu, y = grid_sigma)) +
geom_raster(aes(fill = posterior),
interpolate = T
)+
geom_point(x = valor_x, y = valor_y, size = 1.3,color="white")+
geom_label(
data = punto_comparacion, aes(x, y),
label = "Punto Comparación",
fill = "green",
color = "black",
nudge_y = 0, # Este parametro desplaza la caja por el eje y
nudge_x = 1 # Este parametro desplaza la caja por el eje x
)+
scale_fill_viridis_c() +
labs(
title = "Posterior para Mean y Standard Deviation",
x = expression(mu ["Mean"]),
y = expression(sigma ["Standar Deviation"])
) +
theme(panel.grid = element_blank())
plt
# Codificamos los datos
x <- 1:length(data_grid$posterior)
# Sampleamos los indices
posterior_samples_aux <- sample(x,size = 1e4, replace = T, prob = data_grid$posterior)
# Obtenemos los verdaderos valores de la sampling distribution
posterior_samples <- data_grid[posterior_samples_aux,]
# Obtenemos solos los valores relevantes para la densidad
df <- data.frame(posterior_samples$grid_mu,posterior_samples$grid_sigma)
# Realizamos las densidades
dens(df)
